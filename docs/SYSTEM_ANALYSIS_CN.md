# MDFS Pro (Movie Distributed File System) 系统分析文档

## 一、系统架构概述

# MDFS Pro (Movie Distributed File System) 系统分析文档

## 一、系统架构概述

MDFS Pro 是一个基于 Go 语言构建的**分布式电影存储系统**，采用经典的 Master-Worker 架构。系统通过一致性哈希算法实现数据的分布式存储，配合副本机制保证数据高可用性，并具备故障自愈能力。整个架构设计简洁高效，适用于需要高可用和可扩展存储的中小型分布式应用场景。

系统核心组件包括：Master 节点负责元数据管理、调度、健康监控以及文件操作的协调；Worker 节点负责实际的数据块存储和提供数据服务；一致性哈希环则确保数据的均匀分布和节点的动态伸缩。这种控制平面与数据平面分离的设计，赋予了系统良好的可扩展性和可维护性。

技术栈上，系统使用 Go 1.25.5 作为开发语言，并采用 Go 标准库 `net/http` 构建 RESTful API，力求代码的轻量化和高可控性。数据持久化依赖本地文件系统，并通过 Docker Compose 实现容器化部署，简化了环境配置和部署流程。

## 二、核心功能详解 (分布式特性)

### 2.1 一致性哈希 (Consistent Hashing)

系统通过一致性哈希算法来确定文件的存储位置，利用 `crc32.ChecksumIEEE` 计算文件名哈希值。每个物理 Worker 节点在哈希环上对应 10 个虚拟节点（replicas=10），这一设计有效解决了传统哈希算法中的数据倾斜问题，确保了负载在各 Worker 节点间更加均匀地分布。

当用户上传文件时，Master 节点根据文件名哈希值，在哈希环上顺时针查找，选择第一个节点作为主存储节点，并进一步选择其他节点作为副本节点。这种机制保证了相同的文件名始终映射到相同的存储节点集合，从而实现了数据的可预测性定位和分布式存储。

`consistent_hash.go` 文件实现了完整的哈希环结构，包括节点的添加、哈希值排序和高效的节点查询功能（使用 `sort.Search` 进行二分查找，时间复杂度为 O(log n)），支持返回多个节点地址用于数据副本存储。

### 2.2 数据副本机制与故障自愈

系统默认采用 N=2 的副本策略，即每个文件会在**两个不同的 Worker 节点**上存储一份副本。这一策略不仅显著提升了数据可靠性，还在读取操作时提供了故障转移能力：当一个存储节点暂时不可用时，Master 节点可以自动尝试从文件的另一个副本节点获取数据，保证服务不中断。

**故障自愈功能**由 Master 节点内的 `replicationFixer` 协程实现，该协程每 10 秒周期性运行。它会扫描所有文件的副本状态，如果发现任何文件的副本数量低于期望值（如小于 2），并且系统中存在足够的活跃 Worker 节点，Master 将协调从现有副本节点拉取数据并将其复制到新的可用节点上，从而自动恢复文件的副本完整性。

### 2.3 活跃节点管理与心跳机制

Master 节点通过**心跳机制**持续监控 Worker 节点的健康状态。Worker 节点每 5 秒会向 Master 发送一次心跳请求，并在请求体中携带其当前本地存储的所有文件列表。Master 收到心跳后，会更新该 Worker 节点的活跃时间戳，并同步更新其维护的全局文件索引。

Master 节点还运行一个 `healthChecker` 协程，每 5 秒检查一次所有活跃节点。如果某个 Worker 节点在 15 秒内未发送心跳，Master 会判定该节点失联，将其从活跃节点列表中移除，并同步更新全局文件索引中该节点上存储的文件信息。随后，Master 会重建一致性哈希环，确保新的文件分配请求不会路由到已失联的节点，以此实现集群的动态调整和适应故障。

在 Worker 重新上线并注册时，Master 会检查该 Worker 上是否存在已被标记为删除的文件（墓碑机制），如果存在，则通知 Worker 删除这些陈旧数据，确保数据一致性。

### 2.4 文件上传与分布式分发

文件上传流程经过优化，支持大文件的流式处理，有效避免了对 Master 节点内存的过度占用。具体流程如下：

1.  **权限校验：** Master 节点首先验证上传请求的管理员密钥。
2.  **目标节点确定：** Master 根据文件名使用一致性哈希算法，确定两个目标 Worker 节点用于存储文件及其副本。
3.  **并行数据分发：** Master 节点将接收到的文件数据读取到内存后，为每个目标 Worker 节点创建独立的 Go 协程。每个协程通过 `io.Pipe` 和 `multipart.NewWriter` 构建数据流，并行地将文件内容发送给各自的 Worker 节点。这种设计实现了数据的高效并行传输。
4.  **Worker 存储与校验：** Worker 节点接收文件流，在本地存储文件，并同步计算文件的 CRC32 校验和。校验和会与文件内容一起存储（以 `.checksum` 扩展名），并返回给 Master。
5.  **元数据更新：** Master 接收到 Worker 的成功响应（包含校验和）后，更新全局文件索引和文件的校验和记录。

此流程通过并发传输，提升了文件上传的效率和系统的并行处理能力。

### 2.5 文件下载与流媒体支持

系统支持灵活的文件下载功能，并且对流媒体播放进行了优化：

1.  **故障转移下载：** Master 节点根据全局文件索引，获取文件所有副本所在的 Worker 节点列表。在下载请求到达时，Master 会按顺序尝试从这些 Worker 节点下载数据。一旦某个节点成功响应，Master 就会立即将数据流转发给客户端。这种故障转移策略确保了即使部分 Worker 节点暂时不可用，文件下载服务依然可以稳定运行。
2.  **HTTP Range 支持：** 系统完整支持 HTTP Range 请求。当客户端（如视频播放器）发送带有 `Range` 请求头的下载请求时，Master 会将此请求头透明地透传给 Worker 节点。Worker 节点根据 Range 头返回文件指定字节范围的数据。这一功能是实现视频“拖动进度条”、断点续传等流媒体高级播放体验的关键。
3.  **在线播放 (`/play`)：** 提供专用的 `/play` 接口，通过设置 `Content-Disposition: inline` 头，使浏览器能够直接播放视频文件，支持通过 Web 界面直接观看电影。

### 2.6 数据完整性校验 (CRC32 Checksum)

系统已全面集成 CRC32 校验和机制，以确保存储数据的完整性：

1.  **上传时计算与存储：** 在文件上传至 Worker 节点时，Worker 会即时计算该文件的 CRC32 校验和，并将其以独立的 `.checksum` 文件形式与原始文件一同存储在本地。Master 节点在收到 Worker 的上传成功响应后，也会记录该文件的校验和。
2.  **校验和查询 API (`/checksum` 和 `/get-checksum`)：**
    *   Worker 节点提供 `/checksum?name=xxx` 接口，用于返回其本地存储的指定文件的校验和。
    *   Master 节点提供 `/get-checksum?name=xxx` 接口，优先从其内存中的元数据获取校验和；如果未找到，则会尝试向存储该文件的 Worker 节点查询。
3.  **数据验证 API (`/verify`)：**
    *   Master 节点提供 `/verify?name=xxx` 接口，可以对指定文件进行跨节点校验。它会查询所有存储该文件副本的 Worker 节点，并从它们获取校验和（或执行本地计算），然后将所有节点的校验结果汇总返回。
    *   Worker 节点提供 `/verify?name=xxx&checksum=xxx` 接口，接收 Master 发来的期望校验和，并将其与本地文件的实时计算校验和进行比对，返回比对结果。

这套机制为系统提供了基础的数据完整性保障，能够及时发现数据在传输或存储过程中是否发生损坏。

### 2.7 文件删除与墓碑机制

系统提供了文件删除功能，并引入了墓碑机制以处理分布式环境下的数据一致性问题：

1.  **分布式删除：** Master 节点接收到带有管理员密钥的 `/delete?name=xxx&secret=xxx` 请求后，会并发地向所有存储该文件副本的 Worker 节点发送删除指令。
2.  **Worker 端删除：** Worker 节点收到删除指令后，会从本地文件系统删除文件及其对应的 `.checksum` 文件。
3.  **墓碑记录：** Master 节点在确认文件已从至少一个 Worker 节点成功删除后，会在其内部记录一个文件的“墓碑”（`deletedFiles` map），记录文件名称和删除时间。
4.  **过期墓碑清理：** Master 节点运行一个 `tombstoneCleaner` 协程，定期（每小时）清理超过 30 天的墓碑记录。
5.  **重启节点的数据同步：** 在 Worker 节点重新上线并向 Master 注册时，Master 会检查该 Worker 上报的文件列表中是否存在处于墓碑期（通常为 24 小时内删除）的文件。如果发现此类文件，Master 会主动向该 Worker 发送删除指令，确保已删除的文件不会因 Worker 重启而“复活”。

### 2.8 系统监控与可观测性

MDFS Pro 集成了多个监控端点，支持 Prometheus 等监控系统进行数据采集，提升了系统的可观测性：

1.  **Master 监控接口：**
    *   `/health`：提供基本的健康检查，返回“OK”。
    *   `/stats`：返回 JSON 格式的系统统计信息，包括活跃 Worker 节点数、文件总数、校验和总数和哈希环大小。
    *   `/metrics`：提供 Prometheus 格式的指标，如 `mdfs_active_nodes` (活跃 Worker 节点数)、`mdfs_total_files` (文件总数)、`mdfs_under_replicated_files` (副本不足的文件数) 和 `mdfs_up` (系统运行状态)。
2.  **Worker 监控接口：**
    *   `/health`：提供基本的健康检查，返回“OK”。
    *   `/metrics`：提供 Prometheus 格式的指标，如 `mdfs_worker_files` (本地文件数)、`mdfs_worker_bytes_total` (存储总字节数) 和 `mdfs_worker_up` (Worker 运行状态)。
3.  **Docker Compose 集成：** 项目提供了 `docker-compose.yml` 配置示例，可以轻松部署 Prometheus 和 Grafana，并配置其抓取 Master 和 Worker 节点的 `/metrics` 数据，实现开箱即用的分布式监控仪表盘。

### 2.9 Web 管理控制台

Master 节点提供了一个简洁的基于 Web 的管理控制台，方便管理员进行系统管理和文件操作：

*   **系统概览：** 展示活跃节点数、文件总数、副本不足文件数和副本完整率等核心统计信息。
*   **文件列表：** 列出所有已存储的文件，显示其文件名、部分校验和信息和副本状态。
*   **文件操作：** 管理员登录后（使用硬编码的 `admin888` 密钥），可以在界面上执行文件上传、文件播放、文件下载、文件校验和文件删除等操作。
*   **进度显示：** 上传文件时提供进度条，提升用户体验。
*   **交互式验证：** 提供弹窗显示文件校验结果，清晰展示每个节点的校验状态。

## 三、系统文件结构

```
movie-dist-kv/
├── master/
│   └── master.go          # Master 节点实现（包含元数据管理、调度、健康监控、API处理）
├── worker/
│   └── worker.go          # Worker 节点实现（负责数据存储、心跳汇报、本地文件操作）
├── consistent_hash.go     # 一致性哈希算法实现
├── Dockerfile             # Docker 镜像构建配置
├── docker-compose.yml     # 容器编排配置（含 Master, Worker, Prometheus, Grafana 服务）
├── go.mod                 # Go 模块依赖文件
└── README.md              # 项目说明文档
```

## 四、快速部署

系统可以通过 Docker Compose 快速部署和启动整个集群环境，包括 Master、Worker 节点以及可选的 Prometheus 和 Grafana 监控服务。

```bash
# 启动整个集群（Master, Worker 及其监控组件）
docker-compose up -d

# 查看集群运行日志
docker-compose logs -f

# 停止并移除集群
docker-compose down
```

部署后，可以通过以下地址访问：
*   **Master Web 控制台:** `http://localhost:8080`
*   **Prometheus 监控:** `http://localhost:9090` (如果已部署)
*   **Grafana 仪表盘:** `http://localhost:3000` (如果已部署)

**常用 Master API 监控访问：**
*   健康检查：`curl http://localhost:8080/health`
*   统计信息：`curl http://localhost:8080/stats`
*   Prometheus 指标：`curl http://localhost:8080/metrics`

---

*文档生成时间：2026年1月*
*版本：MDFS Pro v1.0 (已集成数据校验、文件删除、分布式监控等核心功能)*

系统核心组件包括：Master 节点负责元数据管理、调度、健康监控以及文件操作的协调；Worker 节点负责实际的数据块存储和提供数据服务；一致性哈希环则确保数据的均匀分布和节点的动态伸缩。这种控制平面与数据平面分离的设计，赋予了系统良好的可扩展性和可维护性。

技术栈上，系统使用 Go 1.25.5 作为开发语言，并采用 Go 标准库 `net/http` 构建 RESTful API，力求代码的轻量化和高可控性。数据持久化依赖本地文件系统，并通过 Docker Compose 实现容器化部署，简化了环境配置和部署流程。

## 二、核心功能详解 (分布式特性)

### 2.1 一致性哈希 (Consistent Hashing)

系统通过一致性哈希算法来确定文件的存储位置，利用 `crc32.ChecksumIEEE` 计算文件名哈希值。每个物理 Worker 节点在哈希环上对应 10 个虚拟节点（replicas=10），这一设计有效解决了传统哈希算法中的数据倾斜问题，确保了负载在各 Worker 节点间更加均匀地分布。

当用户上传文件时，Master 节点根据文件名哈希值，在哈希环上顺时针查找，选择第一个节点作为主存储节点，并进一步选择其他节点作为副本节点。这种机制保证了相同的文件名始终映射到相同的存储节点集合，从而实现了数据的可预测性定位和分布式存储。

`consistent_hash.go` 文件实现了完整的哈希环结构，包括节点的添加、哈希值排序和高效的节点查询功能（使用 `sort.Search` 进行二分查找，时间复杂度为 O(log n)），支持返回多个节点地址用于数据副本存储。

### 2.2 数据副本机制与故障自愈

系统默认采用 N=2 的副本策略，即每个文件会在**两个不同的 Worker 节点**上存储一份副本。这一策略不仅显著提升了数据可靠性，还在读取操作时提供了故障转移能力：当一个存储节点暂时不可用时，Master 节点可以自动尝试从文件的另一个副本节点获取数据，保证服务不中断。

**故障自愈功能**由 Master 节点内的 `replicationFixer` 协程实现，该协程每 10 秒周期性运行。它会扫描所有文件的副本状态，如果发现任何文件的副本数量低于期望值（如小于 2），并且系统中存在足够的活跃 Worker 节点，Master 将协调从现有副本节点拉取数据并将其复制到新的可用节点上，从而自动恢复文件的副本完整性。

### 2.3 活跃节点管理与心跳机制

Master 节点通过**心跳机制**持续监控 Worker 节点的健康状态。Worker 节点每 5 秒会向 Master 发送一次心跳请求，并在请求体中携带其当前本地存储的所有文件列表。Master 收到心跳后，会更新该 Worker 节点的活跃时间戳，并同步更新其维护的全局文件索引。

Master 节点还运行一个 `healthChecker` 协程，每 5 秒检查一次所有活跃节点。如果某个 Worker 节点在 15 秒内未发送心跳，Master 会判定该节点失联，将其从活跃节点列表中移除，并同步更新全局文件索引中该节点上存储的文件信息。随后，Master 会重建一致性哈希环，确保新的文件分配请求不会路由到已失联的节点，以此实现集群的动态调整和适应故障。

在 Worker 重新上线并注册时，Master 会检查该 Worker 上是否存在已被标记为删除的文件（墓碑机制），如果存在，则通知 Worker 删除这些陈旧数据，确保数据一致性。

### 2.4 文件上传与分布式分发

文件上传流程经过优化，支持大文件的流式处理，有效避免了对 Master 节点内存的过度占用。具体流程如下：

1.  **权限校验：** Master 节点首先验证上传请求的管理员密钥。
2.  **目标节点确定：** Master 根据文件名使用一致性哈希算法，确定两个目标 Worker 节点用于存储文件及其副本。
3.  **并行数据分发：** Master 节点将接收到的文件数据读取到内存后，为每个目标 Worker 节点创建独立的 Go 协程。每个协程通过 `io.Pipe` 和 `multipart.NewWriter` 构建数据流，并行地将文件内容发送给各自的 Worker 节点。这种设计实现了数据的高效并行传输。
4.  **Worker 存储与校验：** Worker 节点接收文件流，在本地存储文件，并同步计算文件的 CRC32 校验和。校验和会与文件内容一起存储（以 `.checksum` 扩展名），并返回给 Master。
5.  **元数据更新：** Master 接收到 Worker 的成功响应（包含校验和）后，更新全局文件索引和文件的校验和记录。

此流程通过并发传输，提升了文件上传的效率和系统的并行处理能力。

### 2.5 文件下载与流媒体支持

系统支持灵活的文件下载功能，并且对流媒体播放进行了优化：

1.  **故障转移下载：** Master 节点根据全局文件索引，获取文件所有副本所在的 Worker 节点列表。在下载请求到达时，Master 会按顺序尝试从这些 Worker 节点下载数据。一旦某个节点成功响应，Master 就会立即将数据流转发给客户端。这种故障转移策略确保了即使部分 Worker 节点暂时不可用，文件下载服务依然可以稳定运行。
2.  **HTTP Range 支持：** 系统完整支持 HTTP Range 请求。当客户端（如视频播放器）发送带有 `Range` 请求头的下载请求时，Master 会将此请求头透明地透传给 Worker 节点。Worker 节点根据 Range 头返回文件指定字节范围的数据。这一功能是实现视频“拖动进度条”、断点续传等流媒体高级播放体验的关键。
3.  **在线播放 (`/play`)：** 提供专用的 `/play` 接口，通过设置 `Content-Disposition: inline` 头，使浏览器能够直接播放视频文件，支持通过 Web 界面直接观看电影。

### 2.6 数据完整性校验 (CRC32 Checksum)

系统已全面集成 CRC32 校验和机制，以确保存储数据的完整性：

1.  **上传时计算与存储：** 在文件上传至 Worker 节点时，Worker 会即时计算该文件的 CRC32 校验和，并将其以独立的 `.checksum` 文件形式与原始文件一同存储在本地。Master 节点在收到 Worker 的上传成功响应后，也会记录该文件的校验和。
2.  **校验和查询 API (`/checksum` 和 `/get-checksum`)：**
    *   Worker 节点提供 `/checksum?name=xxx` 接口，用于返回其本地存储的指定文件的校验和。
    *   Master 节点提供 `/get-checksum?name=xxx` 接口，优先从其内存中的元数据获取校验和；如果未找到，则会尝试向存储该文件的 Worker 节点查询。
3.  **数据验证 API (`/verify`)：**
    *   Master 节点提供 `/verify?name=xxx` 接口，可以对指定文件进行跨节点校验。它会查询所有存储该文件副本的 Worker 节点，并从它们获取校验和（或执行本地计算），然后将所有节点的校验结果汇总返回。
    *   Worker 节点提供 `/verify?name=xxx&checksum=xxx` 接口，接收 Master 发来的期望校验和，并将其与本地文件的实时计算校验和进行比对，返回比对结果。

这套机制为系统提供了基础的数据完整性保障，能够及时发现数据在传输或存储过程中是否发生损坏。

### 2.7 文件删除与墓碑机制

系统提供了文件删除功能，并引入了墓碑机制以处理分布式环境下的数据一致性问题：

1.  **分布式删除：** Master 节点接收到带有管理员密钥的 `/delete?name=xxx&secret=xxx` 请求后，会并发地向所有存储该文件副本的 Worker 节点发送删除指令。
2.  **Worker 端删除：** Worker 节点收到删除指令后，会从本地文件系统删除文件及其对应的 `.checksum` 文件。
3.  **墓碑记录：** Master 节点在确认文件已从至少一个 Worker 节点成功删除后，会在其内部记录一个文件的“墓碑”（`deletedFiles` map），记录文件名称和删除时间。
4.  **过期墓碑清理：** Master 节点运行一个 `tombstoneCleaner` 协程，定期（每小时）清理超过 30 天的墓碑记录。
5.  **重启节点的数据同步：** 在 Worker 节点重新上线并向 Master 注册时，Master 会检查该 Worker 上报的文件列表中是否存在处于墓碑期（通常为 24 小时内删除）的文件。如果发现此类文件，Master 会主动向该 Worker 发送删除指令，确保已删除的文件不会因 Worker 重启而“复活”。

### 2.8 系统监控与可观测性

MDFS Pro 集成了多个监控端点，支持 Prometheus 等监控系统进行数据采集，提升了系统的可观测性：

1.  **Master 监控接口：**
    *   `/health`：提供基本的健康检查，返回“OK”。
    *   `/stats`：返回 JSON 格式的系统统计信息，包括活跃 Worker 节点数、文件总数、校验和总数和哈希环大小。
    *   `/metrics`：提供 Prometheus 格式的指标，如 `mdfs_active_nodes` (活跃 Worker 节点数)、`mdfs_total_files` (文件总数)、`mdfs_under_replicated_files` (副本不足的文件数) 和 `mdfs_up` (系统运行状态)。
2.  **Worker 监控接口：**
    *   `/health`：提供基本的健康检查，返回“OK”。
    *   `/metrics`：提供 Prometheus 格式的指标，如 `mdfs_worker_files` (本地文件数)、`mdfs_worker_bytes_total` (存储总字节数) 和 `mdfs_worker_up` (Worker 运行状态)。
3.  **Docker Compose 集成：** 项目提供了 `docker-compose.yml` 配置示例，可以轻松部署 Prometheus 和 Grafana，并配置其抓取 Master 和 Worker 节点的 `/metrics` 数据，实现开箱即用的分布式监控仪表盘。

### 2.9 Web 管理控制台

Master 节点提供了一个简洁的基于 Web 的管理控制台，方便管理员进行系统管理和文件操作：

*   **系统概览：** 展示活跃节点数、文件总数、副本不足文件数和副本完整率等核心统计信息。
*   **文件列表：** 列出所有已存储的文件，显示其文件名、部分校验和信息和副本状态。
*   **文件操作：** 管理员登录后（使用硬编码的 `admin888` 密钥），可以在界面上执行文件上传、文件播放、文件下载、文件校验和文件删除等操作。
*   **进度显示：** 上传文件时提供进度条，提升用户体验。
*   **交互式验证：** 提供弹窗显示文件校验结果，清晰展示每个节点的校验状态。

## 三、系统文件结构

```
movie-dist-kv/
├── master/
│   └── master.go          # Master 节点实现（包含元数据管理、调度、健康监控、API处理）
├── worker/
│   └── worker.go          # Worker 节点实现（负责数据存储、心跳汇报、本地文件操作）
├── consistent_hash.go     # 一致性哈希算法实现
├── Dockerfile             # Docker 镜像构建配置
├── docker-compose.yml     # 容器编排配置（含 Master, Worker, Prometheus, Grafana 服务）
├── go.mod                 # Go 模块依赖文件
└── README.md              # 项目说明文档
```

## 四、快速部署

系统可以通过 Docker Compose 快速部署和启动整个集群环境，包括 Master、Worker 节点以及可选的 Prometheus 和 Grafana 监控服务。

```bash
# 启动整个集群（Master, Worker 及其监控组件）
docker-compose up -d

# 查看集群运行日志
docker-compose logs -f

# 停止并移除集群
docker-compose down
```

部署后，可以通过以下地址访问：
*   **Master Web 控制台:** `http://localhost:8080`
*   **Prometheus 监控:** `http://localhost:9090` (如果已部署)
*   **Grafana 仪表盘:** `http://localhost:3000` (如果已部署)

**常用 Master API 监控访问：**
*   健康检查：`curl http://localhost:8080/health`
*   统计信息：`curl http://localhost:8080/stats`
*   Prometheus 指标：`curl http://localhost:8080/metrics`

---

*文档生成时间：2026年1月*
*版本：MDFS Pro v1.0 (已集成数据校验、文件删除、分布式监控等核心功能)*